import comp from "/media/goat/data/project/Blog/myBlog/src/.vuepress/.temp/pages/ai-ml/RL/01强化学习概述.html.vue"
const data = JSON.parse("{\"path\":\"/ai-ml/RL/01%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0.html\",\"title\":\"强化学习概述\",\"lang\":\"zh-CN\",\"frontmatter\":{\"icon\":\"0\",\"date\":\"2026-01-19T00:00:00.000Z\",\"order\":1,\"title\":\"强化学习概述\",\"category\":[\"强化学习\"],\"tag\":[\"概述\"],\"description\":\"强化学习概述 强化学习是智能体为了最大化长期回报的期望，通过观察系统花鸟卷，不断试错进行学习的过程。 属于机器学习的第三范式，与监督学习、无监督学习并列。 发展史 试错学习 最优控制，寻求使给定系统的性能达到最优的控制 时序差分法，由两个等间隔相邻时间段的预测差值驱动的学习过程 深度强化学习（现代） 分类 表格求解法（Tabular） 解决状态和行动空...\",\"head\":[[\"script\",{\"type\":\"application/ld+json\"},\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Article\\\",\\\"headline\\\":\\\"强化学习概述\\\",\\\"image\\\":[\\\"\\\"],\\\"datePublished\\\":\\\"2026-01-19T00:00:00.000Z\\\",\\\"dateModified\\\":null,\\\"author\\\":[{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"Goat_Yang\\\",\\\"url\\\":\\\"../intro.html\\\"}]}\"],[\"meta\",{\"property\":\"og:url\",\"content\":\"https://github.com/Yang-goat/MyBlog/ai-ml/RL/01%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0.html\"}],[\"meta\",{\"property\":\"og:site_name\",\"content\":\"Goat_Yang\"}],[\"meta\",{\"property\":\"og:title\",\"content\":\"强化学习概述\"}],[\"meta\",{\"property\":\"og:description\",\"content\":\"强化学习概述 强化学习是智能体为了最大化长期回报的期望，通过观察系统花鸟卷，不断试错进行学习的过程。 属于机器学习的第三范式，与监督学习、无监督学习并列。 发展史 试错学习 最优控制，寻求使给定系统的性能达到最优的控制 时序差分法，由两个等间隔相邻时间段的预测差值驱动的学习过程 深度强化学习（现代） 分类 表格求解法（Tabular） 解决状态和行动空...\"}],[\"meta\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"meta\",{\"property\":\"og:locale\",\"content\":\"zh-CN\"}],[\"meta\",{\"property\":\"article:tag\",\"content\":\"概述\"}],[\"meta\",{\"property\":\"article:published_time\",\"content\":\"2026-01-19T00:00:00.000Z\"}]]},\"git\":{},\"readingTime\":{\"minutes\":2.53,\"words\":760},\"filePathRelative\":\"ai-ml/RL/01强化学习概述.md\",\"excerpt\":\"\\n<p>强化学习是智能体为了最大化长期回报的期望，通过观察系统花鸟卷，不断试错进行学习的过程。</p>\\n<p>属于机器学习的第三范式，与监督学习、无监督学习并列。</p>\\n<h2>发展史</h2>\\n<ol>\\n<li>试错学习</li>\\n<li>最优控制，寻求使给定系统的性能达到最优的控制</li>\\n<li>时序差分法，由两个等间隔相邻时间段的预测差值驱动的学习过程</li>\\n<li>深度强化学习（现代）</li>\\n</ol>\\n<h2>分类</h2>\\n<ol>\\n<li>\\n<p><strong>表格求解法</strong>（Tabular）</p>\\n<ul>\\n<li>解决状态和行动空间足够小并可用有限大小数组和表进行标识的强化学习问题</li>\\n<li>动态规划法（Dynamic Programming (DP) Methods）</li>\\n<li>蒙特卡洛法（Monte Carlo Methods）</li>\\n<li>时序差分法（Temporal-Difference (TD) Learning）</li>\\n<li>异策略时序差分 Q-Learning 算法</li>\\n</ul>\\n</li>\\n<li>\\n<p><strong>近似求解法</strong>（Approximate）</p>\\n<ul>\\n<li>解决连续状态或行动空间</li>\\n<li>值函数近似法（Value Function Approximation）</li>\\n<li>策略梯度法（Policy Gradient Methods）</li>\\n<li>深度强化学习（Deep RL）</li>\\n</ul>\\n</li>\\n<li>\\n<p>有模型</p>\\n<ul>\\n<li>动态规划法</li>\\n</ul>\\n</li>\\n<li>\\n<p>无模型</p>\\n<ul>\\n<li>蒙特卡洛</li>\\n<li>时序差分</li>\\n<li>异策略学习</li>\\n<li>值函数近似</li>\\n<li>策略梯度</li>\\n<li>深度强化学习</li>\\n</ul>\\n</li>\\n</ol>\",\"autoDesc\":true}")
export { comp, data }

if (import.meta.webpackHot) {
  import.meta.webpackHot.accept()
  if (__VUE_HMR_RUNTIME__.updatePageData) {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  }
}

if (import.meta.hot) {
  import.meta.hot.accept(({ data }) => {
    __VUE_HMR_RUNTIME__.updatePageData(data)
  })
}
