---
icon: star
date: 2026-01-19
order: 11
title: 总结
category:
  - 强化学习
# tag:
#   - 概述
---

# 总结

## 强化学习脉络思维导图

```markmap
# Reinforcement Learning

## 0. Problem Formulation
### Markov Decision Process (MDP)
- State S
- Action A
- Transition P(s'|s,a)
- Reward R(s,a)
- Policy π(a|s)
- Discount γ

### Trajectory
- (s0, a0, r1, s1, ...)

### Return
- G_t = r_{t+1} + γ r_{t+2} + γ² r_{t+3} + ...

---

## 1. Value Theory
### State Value
- v_π(s) = E[G_t | s]

### Action Value
- q_π(s,a) = E[G_t | s,a]

### Bellman Expectation Equation
- v(s) = E[r + γ v(s')]

### Bellman Optimality Equation
- v(s) = max_a E[r + γ v(s')]

---

## 2. Model-Based Methods (Dynamic Programming)
### Policy Evaluation
- v(s) ← E[r + γ v(s')]

### Policy Improvement
- π(s) ← argmax_a q(s,a)

### Policy Iteration
- PE ↔ PI

### Value Iteration
- v(s) ← max_a E[r + γ v(s')]

---

## 3. Model-Free Methods

### 3.1 Monte Carlo (No bootstrapping)
#### Core Idea
- Learn from complete episodes

#### Update
- Q ← Q + α (G - Q)

#### Variants
- First-visit
- Every-visit
- Exploring Starts
- On-policy MC
- Off-policy MC

---

### 3.2 Temporal Difference (Bootstrapping)

#### TD(0)
- v(s_t) ← v(s_t) + α [r_t + γ v(s_{t+1}) - v(s_t)]

#### n-step TD
- Mix MC and TD

#### TD(λ)
- Eligibility trace

---

### 3.3 Control with TD

#### SARSA (On-policy)
- Q ← Q + α [r + γ Q(s',a') - Q(s,a)]

#### Q-learning (Off-policy)
- Q ← Q + α [r + γ max_a Q(s',a) - Q(s,a)]

#### Expected SARSA
- Expectation over π

---

## 4. Stochastic Approximation View
### Root Finding
- Solve g(w)=0

### Robbins-Monro
- w ← w - α ĝ(w)

### TD as SA
- Bellman residual = 0

### SGD Interpretation
- All RL ≈ noisy gradient descent

---

## 5. Value Function Approximation

### Motivation
- Large / continuous state space

### Linear Approximation
- v̂(s) = φ(s)^T w

### Nonlinear Approximation
- Neural networks

### Objective
- min E[(v̂(s) - v(s))²]

### Stability Issues
- Deadly Triad:
  - Bootstrapping
  - Off-policy
  - Function approximation

---

## 6. Policy-Based Methods

### Core Idea
- Directly optimize policy

### Parameterization
- π(a|s,θ)

### Objective
- J(θ) = E[G]

### REINFORCE
- θ ← θ + α ∇θ log π(a|s) G

### Variance Reduction
- Baseline
- Advantage

---

## 7. Actor-Critic Framework

### Motivation
- Combine value-based and policy-based

### Actor
- Policy π(a|s,θ)

### Critic
- Value estimator

### Core Update

#### Critic
- δ = r + γ V(s') - V(s)

#### Actor
- θ ← θ + α ∇θ log π(a|s) · signal

---

### Classic Actor-Critic

#### Q Actor-Critic
- Signal = Q(s,a)

#### Advantage Actor-Critic (A2C)
- A(s,a) = Q(s,a) - V(s)

#### A3C
- Asynchronous

---

## 8. Deep Actor-Critic Family

### Deterministic Policy
#### DDPG
- μ(s|θ)

#### TD3
- Double critic
- Delayed update

---

### Maximum Entropy
#### SAC
- J = E[Q(s,a) + α H(π)]

---

### Trust Region
#### TRPO
- KL constraint

#### PPO
- Clipped objective

---

## 9. Modern DQN Family

### DQN
- Q-learning + NN

### Double DQN
- Reduce overestimation

### Dueling DQN
- V(s) + A(s,a)

### Prioritized Replay
- Better sampling

### Rainbow
- Everything combined

---

## 10. Exploration

### Random
- ε-greedy

### Softmax

### UCB

### Intrinsic Motivation
- Curiosity
- RND

---

## 11. Core Design Dimensions

### Model-based vs Model-free

### On-policy vs Off-policy

### Bootstrapping vs Monte Carlo

### Tabular vs Approximation

### Discrete vs Continuous

---

## 12. Unifying View

### Bellman Equation
- Root of everything

### Value-based
- Solve Bellman

### Policy-based
- Optimize J(θ)

### Actor-Critic
- Hybrid system

### Deep RL
- Function approximation everywhere
```

## 方法对比矩阵

| Method        | Model-based | Policy-based | Value-based | Actor-Critic | On-policy | Off-policy | Bootstrapping | Continuous Action | Function Approx | Stable |
|--------------|-------------|--------------|-------------|--------------|-----------|------------|---------------|-------------------|-----------------|--------|
| Policy Iteration | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ✅ |
| Value Iteration  | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ✅ |
| Monte Carlo      | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ | ❌ | ❌ |
| TD(0)            | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| SARSA            | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |
| Q-learning       | ❌ | ❌ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | ❌ | ❌ |
| DQN              | ❌ | ❌ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | ✅ | ❌ |
| REINFORCE        | ❌ | ✅ | ❌ | ❌ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ |
| Actor-Critic     | ❌ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ❌ |
| A2C              | ❌ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ❌ |
| A3C              | ❌ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ❌ |
| PPO              | ❌ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |
| DDPG             | ❌ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ | ❌ |
| TD3              | ❌ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ | ❌ |
| SAC              | ❌ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ | ✅ |

## 核心公式速查表

### 1. Dynamic Programming (Model-based)

#### Policy Evaluation
$$
v(s) \leftarrow \sum_a \pi(a|s)\Big(r(s,a) + \gamma \sum_{s'} P(s'|s,a)\, v(s')\Big)
$$

#### Value Iteration
$$
v(s) \leftarrow \max_a \Big(r(s,a) + \gamma \sum_{s'} P(s'|s,a)\, v(s')\Big)
$$

---

### 2. Monte Carlo

#### MC Policy Evaluation
$$
Q(s,a) \leftarrow Q(s,a) + \alpha\big(G_t - Q(s,a)\big)
$$

---

### 3. Temporal Difference

#### TD(0)
$$
V(s_t) \leftarrow V(s_t) + \alpha \Big(r_t + \gamma V(s_{t+1}) - V(s_t)\Big)
$$

#### n-step TD
$$
V(s_t) \leftarrow V(s_t) + \alpha \Big(G_t^{(n)} - V(s_t)\Big)
$$

---

### 4. Control with TD

#### SARSA (On-policy)
$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \Big(r_t + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\Big)
$$

#### Q-learning (Off-policy)
$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \Big(r_t + \gamma \max_a Q(s_{t+1},a) - Q(s_t,a_t)\Big)
$$

#### Expected SARSA
$$
Q \leftarrow Q + \alpha \Big(r + \gamma \mathbb{E}_{a' \sim \pi}[Q(s',a')] - Q\Big)
$$

---

### 5. Policy Gradient

#### REINFORCE
$$
\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t)\, G_t
$$

---

### 6. Actor-Critic

#### Critic (TD Error)
$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

#### Actor
$$
\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi(a_t|s_t)\, \delta_t
$$

---

### 7. Advantage Actor-Critic (A2C)

#### Advantage
$$
A(s,a) = Q(s,a) - V(s)
$$

#### Actor Update
$$
\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi(a|s)\, A(s,a)
$$

---

### 8. Deep Q-Network (DQN)

#### Loss
$$
L(\theta) = \Big(r + \gamma \max_{a'} Q(s',a';\theta^-) - Q(s,a;\theta)\Big)^2
$$

---

### 9. PPO (Clipped Objective)

#### Probability Ratio
$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
$$

#### Objective
$$
L(\theta) = \mathbb{E}\Big[\min(r_t(\theta)A_t,\; \text{clip}(r_t(\theta),1-\epsilon,1+\epsilon)A_t)\Big]
$$

---

### 10. SAC (Maximum Entropy RL)

#### Soft Q Target
$$
y = r + \gamma \Big(V(s') - \alpha \log \pi(a'|s')\Big)
$$

#### Q Loss
$$
J_Q = \mathbb{E}\big[(Q(s,a) - y)^2\big]
$$

#### Policy Loss
$$
J_\pi = \mathbb{E}\big[\alpha \log \pi(a|s) - Q(s,a)\big]
$$

## 工程技巧核心公式表

### 1. Target Network

#### Purpose
- Stabilize bootstrapping

#### Formula
$$
y = r + \gamma \max_{a'} Q(s', a'; \theta^-)
$$

Where:
- $\theta^-$ = slow-moving target parameters

---

### 2. Experience Replay

#### Purpose
- Break correlation
- Improve sample efficiency

#### Update (same as Q-learning)
$$
Q(s,a) \leftarrow Q(s,a) + \alpha \big(y - Q(s,a)\big)
$$

Where:
$$
(s,a,r,s') \sim \mathcal{D}
$$

---

### 3. Advantage Normalization

#### Purpose
- Reduce variance
- Stabilize updates

#### Formula
$$
\hat{A}_t = \frac{A_t - \mu_A}{\sigma_A + \epsilon}
$$

---

### 4. Generalized Advantage Estimation (GAE)

#### TD Error
$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

#### GAE
$$
A_t^{(\lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
$$

---

### 5. Entropy Bonus

#### Purpose
- Encourage exploration

#### Objective
$$
J(\theta) = J_{\text{RL}}(\theta) + \beta \mathcal{H}(\pi(\cdot|s))
$$

Where:
$$
\mathcal{H}(\pi) = -\sum_a \pi(a|s) \log \pi(a|s)
$$

---

### 6. Clipped Objective (PPO)

#### Probability Ratio
$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
$$

#### Clipped Loss
$$
L(\theta) = \mathbb{E}\Big[\min(r_t A_t,\; \text{clip}(r_t, 1-\epsilon, 1+\epsilon) A_t)\Big]
$$

---

### 7. Soft Target Update

#### Purpose
- Prevent divergence

#### Formula
$$
\theta^- \leftarrow \tau \theta + (1-\tau)\theta^-
$$

---

### 8. Reward Normalization

#### Purpose
- Stabilize learning

#### Formula
$$
\hat{r}_t = \frac{r_t - \mu_r}{\sigma_r + \epsilon}
$$

---

### 9. Value Clipping (PPO)

#### Formula
$$
V_{\text{clip}} = V_{\text{old}} + \text{clip}(V - V_{\text{old}}, -\epsilon, \epsilon)
$$

---

### 10. Double Q-learning

#### Purpose
- Reduce overestimation

#### Formula
$$
y = r + \gamma Q_{\theta^-}(s', \arg\max_a Q_\theta(s',a))
$$

