---
icon: "0"
date: 2026-01-19
order: 1
title: 强化学习概述
category:
  - 强化学习
tag:
  - 概述
---

# 强化学习概述

强化学习是智能体为了最大化长期回报的期望，通过观察系统花鸟卷，不断试错进行学习的过程。

属于机器学习的第三范式，与监督学习、无监督学习并列。

## 发展史

1.  试错学习
2.  最优控制，寻求使给定系统的性能达到最优的控制
3.  时序差分法，由两个等间隔相邻时间段的预测差值驱动的学习过程
4.  深度强化学习（现代）

## 分类

1.  **表格求解法**（Tabular）

    *   解决状态和行动空间足够小并可用有限大小数组和表进行标识的强化学习问题
    *   动态规划法（Dynamic Programming (DP) Methods）
    *   蒙特卡洛法（Monte Carlo Methods）
    *   时序差分法（Temporal-Difference (TD) Learning）
    *   异策略时序差分 Q-Learning 算法

2.  **近似求解法**（Approximate）

    *   解决连续状态或行动空间
    *   值函数近似法（Value Function Approximation）
    *   策略梯度法（Policy Gradient Methods）
    *   深度强化学习（Deep RL）

1.  有模型

    *   动态规划法

2.  无模型

    *   蒙特卡洛
    *   时序差分
    *   异策略学习
    *   值函数近似
    *   策略梯度
    *   深度强化学习

***

## 基础概念

*   State：$s_1,s_2,\cdots,s_9$

    *   State space： $\mathcal{S}=\{s_i\}^9_{i=1}$

*   Action：$a_1,\cdots,a_5$

    *   Action space of a state：一个状态的可能的行动的集合， $\mathcal{A}(s_i)=\{a_i\}^5_{i=1}$

*   state transition：

    *   $$
        s_1 \xrightarrow{a_2} s_2
        $$
    *   State transition probability：状态转移概率（条件概率）
    *   $$
        p(s_2|s_1,a_2)=1 \\
        p(s_i|s_1,a_2)=0 \ \ \ \forall i \neq 2
        $$

*   Policy：告诉智能体在一个状态下应该采取什么行动

    *   $$
        \pi(a_1|s_1)=0 \\
        \pi(a_2|s_1)=0.5 \\
        \pi(a_3|s_1)=0.5 \\
        \pi(a_4|s_1)=0 \\
        \pi(a_5|s_1)=0 \\
        $$

*   Reward：实数，reward transition is stochastic（随机的）

    *   $$
        p(r=-1|s_1,a_1)=1 \\
        p(r\neq-1|s_1,a_1)=0
        $$

*   Trajectory：轨迹，状态动作奖励链

    *   $$
        s_1 \xrightarrow[\;r=0\;]{a_2}
        s_2 \xrightarrow[\;r=0\;]{a_3}
        s_5 \xrightarrow[\;r=0\;]{a_3}
        s_8 \xrightarrow[\;r=1\;]{a_2}
        s_9
        $$

*   Return

    *   $$
        \text{return} = 0+0+0+1=1
        $$

*   Discounted return：贴现

    *   discounted rate： $\gamma\in[0,1)$

    *   $$
        \begin{aligned}
        \text{discounted return}
        &= 0 + \gamma 0 + \gamma^2 0 + \gamma^3 1 + \gamma^4 1 + \gamma^5 1 + \cdots \\
        &= \gamma^3 \left( 1 + \gamma + \gamma^2 + \cdots \right) \\
        &= \gamma^3 \frac{1}{1 - \gamma}.
        \end{aligned}
        $$
    
    *   $\gamma$  越小越短视
    
    *   $\gamma$  越大越远视

*   Episode（trial）：回合

    *   在 terminal states (终止条件) 时停止一个回合

### terminal tasks 与 continuing tasks

可将 terminal tasks 转化为 continuing tasks

转换方法

1.  treat the target state as a special absorbing state（只能自己转移到自己，且  $r=0$ ）

2.  treat the target state as a normal state with a policy（可以离开，当进入时  $r=+1$ ）

***

## Markov decision process (MDP)

关键元素

*   Sets

    *   State: the set of states  $\mathcal{S}$

    *   Action: the set of actions  $\mathcal{A}(s)$  is associated for state  $s\in\mathcal{S}$

    *   Reward: the set of rewards  $\mathcal{R}(s,a)$

*   Probability distribution

    *   State transition probability： $p(s'|s,a)$

    *   Reward probability： $p(r|s,a)$

*   Policy： $\pi(a|s)$

*   *Markov property*：**memoryless **property

    *   $$
        p(s_{t+1}\mid a_t, s_t, \ldots, a_0, s_0)= p(s_{t+1}\mid a_t, s_t), \\
        p(r_{t+1}\mid a_t, s_t, \ldots, a_0, s_0)= p(r_{t+1}\mid a_t, s_t).
        $$

当策略给定时，MDP变成马尔可夫过程
